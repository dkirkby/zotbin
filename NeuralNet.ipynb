{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classifier Random\n",
      "Found classifier ZotBin\n",
      "Found classifier RandomForest\n",
      "Found classifier IBandOnly\n"
     ]
    }
   ],
   "source": [
    "from tomo_challenge import load_data, load_redshift\n",
    "from tomo_challenge.jax_metrics import ell_binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nn, optim, serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize fast metric calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zotbin.binned import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data = load_binned('binned_40.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the challenge data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands='riz'\n",
    "include_colors=False\n",
    "include_errors=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5410171 training rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkirkby/DESC/tomo/tomo_challenge/data.py:89: UserWarning: Setting inf (undetected) bands to mag=30\n",
      "  warnings.warn(\"Setting inf (undetected) bands to mag=30\")\n"
     ]
    }
   ],
   "source": [
    "train_file='/media/data2/tomo_challenge_data/ugrizy_buzzard/training.hdf5'\n",
    "train_data = load_data(train_file, bands, \n",
    "                       errors=include_errors,\n",
    "                       colors=include_colors, array=True)\n",
    "train_z = load_redshift(train_file)\n",
    "print(f'Loaded {len(train_data)} training rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.diff(train_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iband = bands.index('i')\n",
    "data = np.concatenate((colors, train_data[:, iband:iband+1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = RobustScaler()\n",
    "features = preproc.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load classification labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2 = np.load('buzzard_labels2.npy')\n",
    "label4 = np.load('buzzard_labels4.npy')\n",
    "label8 = np.load('buzzard_labels8.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    plt.hist(train_z[label8 == i], bins=np.linspace(0, 3, 100), alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dense classifier network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def apply(self, x):\n",
    "        x = nn.Dense(x, features=100, name='L1')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, features=100, name='L2')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, features=8, name='L3')\n",
    "        x = nn.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def cross_entropy_loss(logits, label):\n",
    "    return -logits[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(optimizer, batch):\n",
    "    def loss_fn(model):\n",
    "        logits = model(batch['features'])\n",
    "        loss = jnp.mean(cross_entropy_loss(logits, batch['labels']))\n",
    "        return loss\n",
    "    grad = jax.grad(loss_fn)(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(grad)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(model, eval_ds):\n",
    "    logits = model(eval_ds['features'])\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == eval_ds['labels'])\n",
    "    loss = jnp.mean(cross_entropy_loss(logits, eval_ds['labels']))\n",
    "    return {'loss': loss,  'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1': {'bias': (100,), 'kernel': (3, 100)}, 'L2': {'bias': (100,), 'kernel': (100, 100)}, 'L3': {'bias': (8,), 'kernel': (100, 8)}}\n",
      "epoch 1/100: train loss=1.549593 accuracy=0.381428\n",
      "epoch 2/100: train loss=1.410634 accuracy=0.463285\n",
      "epoch 3/100: train loss=1.339616 accuracy=0.466406\n",
      "epoch 4/100: train loss=1.301584 accuracy=0.475462\n",
      "epoch 5/100: train loss=1.279500 accuracy=0.479665\n",
      "epoch 6/100: train loss=1.280414 accuracy=0.469842\n",
      "epoch 7/100: train loss=1.274264 accuracy=0.475109\n",
      "epoch 8/100: train loss=1.246219 accuracy=0.481805\n",
      "epoch 9/100: train loss=1.231485 accuracy=0.489400\n",
      "epoch 10/100: train loss=1.229690 accuracy=0.488708\n",
      "epoch 11/100: train loss=1.222467 accuracy=0.494511\n",
      "epoch 12/100: train loss=1.212283 accuracy=0.499441\n",
      "epoch 13/100: train loss=1.207327 accuracy=0.501725\n",
      "epoch 14/100: train loss=1.214399 accuracy=0.491900\n",
      "epoch 15/100: train loss=1.190291 accuracy=0.514734\n",
      "epoch 16/100: train loss=1.184384 accuracy=0.512024\n",
      "epoch 17/100: train loss=1.184523 accuracy=0.516476\n",
      "epoch 18/100: train loss=1.174873 accuracy=0.518772\n",
      "epoch 19/100: train loss=1.187987 accuracy=0.510886\n",
      "epoch 20/100: train loss=1.159245 accuracy=0.524206\n",
      "epoch 21/100: train loss=1.149853 accuracy=0.534554\n",
      "epoch 22/100: train loss=1.141510 accuracy=0.538966\n",
      "epoch 23/100: train loss=1.139448 accuracy=0.530606\n",
      "epoch 24/100: train loss=1.128952 accuracy=0.545597\n",
      "epoch 25/100: train loss=1.123110 accuracy=0.550583\n",
      "epoch 26/100: train loss=1.136176 accuracy=0.533236\n",
      "epoch 27/100: train loss=1.115480 accuracy=0.553310\n",
      "epoch 28/100: train loss=1.121037 accuracy=0.550099\n",
      "epoch 29/100: train loss=1.105724 accuracy=0.554321\n",
      "epoch 30/100: train loss=1.097084 accuracy=0.560326\n",
      "epoch 31/100: train loss=1.106882 accuracy=0.544057\n",
      "epoch 32/100: train loss=1.098094 accuracy=0.563017\n",
      "epoch 33/100: train loss=1.092574 accuracy=0.561275\n",
      "epoch 34/100: train loss=1.083743 accuracy=0.561175\n",
      "epoch 35/100: train loss=1.079934 accuracy=0.570059\n",
      "epoch 36/100: train loss=1.072775 accuracy=0.572536\n",
      "epoch 37/100: train loss=1.069658 accuracy=0.575205\n",
      "epoch 38/100: train loss=1.079702 accuracy=0.564128\n",
      "epoch 39/100: train loss=1.063540 accuracy=0.574011\n",
      "epoch 40/100: train loss=1.064687 accuracy=0.571221\n",
      "epoch 41/100: train loss=1.067374 accuracy=0.557335\n",
      "epoch 42/100: train loss=1.054654 accuracy=0.574067\n",
      "epoch 43/100: train loss=1.052411 accuracy=0.579252\n",
      "epoch 44/100: train loss=1.050763 accuracy=0.575362\n",
      "epoch 45/100: train loss=1.046976 accuracy=0.576925\n",
      "epoch 46/100: train loss=1.078957 accuracy=0.531430\n",
      "epoch 47/100: train loss=1.048617 accuracy=0.573484\n",
      "epoch 48/100: train loss=1.077685 accuracy=0.544805\n",
      "epoch 49/100: train loss=1.038510 accuracy=0.582211\n",
      "epoch 50/100: train loss=1.048394 accuracy=0.564864\n",
      "epoch 51/100: train loss=1.037143 accuracy=0.579281\n",
      "epoch 52/100: train loss=1.041452 accuracy=0.571278\n",
      "epoch 53/100: train loss=1.038629 accuracy=0.569874\n",
      "epoch 54/100: train loss=1.034149 accuracy=0.575269\n",
      "epoch 55/100: train loss=1.033359 accuracy=0.576211\n",
      "epoch 56/100: train loss=1.027123 accuracy=0.582302\n",
      "epoch 57/100: train loss=1.025657 accuracy=0.583969\n",
      "epoch 58/100: train loss=1.033779 accuracy=0.572509\n",
      "epoch 59/100: train loss=1.026169 accuracy=0.577068\n",
      "epoch 60/100: train loss=1.028821 accuracy=0.572441\n",
      "epoch 61/100: train loss=1.021134 accuracy=0.584496\n",
      "epoch 62/100: train loss=1.032546 accuracy=0.581965\n",
      "epoch 63/100: train loss=1.031148 accuracy=0.572799\n",
      "epoch 64/100: train loss=1.018808 accuracy=0.585183\n",
      "epoch 65/100: train loss=1.021231 accuracy=0.585461\n",
      "epoch 66/100: train loss=1.021694 accuracy=0.577675\n",
      "epoch 67/100: train loss=1.020069 accuracy=0.580469\n",
      "epoch 68/100: train loss=1.016405 accuracy=0.587281\n",
      "epoch 69/100: train loss=1.015959 accuracy=0.583623\n",
      "epoch 70/100: train loss=1.015362 accuracy=0.580591\n",
      "epoch 71/100: train loss=1.017017 accuracy=0.580248\n",
      "epoch 72/100: train loss=1.013063 accuracy=0.587836\n",
      "epoch 73/100: train loss=1.011292 accuracy=0.584743\n",
      "epoch 74/100: train loss=1.013893 accuracy=0.586811\n",
      "epoch 75/100: train loss=1.018653 accuracy=0.584736\n",
      "epoch 76/100: train loss=1.011438 accuracy=0.586584\n",
      "epoch 77/100: train loss=1.008028 accuracy=0.585996\n",
      "epoch 78/100: train loss=1.008804 accuracy=0.588428\n",
      "epoch 79/100: train loss=1.008035 accuracy=0.588062\n",
      "epoch 80/100: train loss=1.019251 accuracy=0.583500\n",
      "epoch 81/100: train loss=1.008522 accuracy=0.588245\n",
      "epoch 82/100: train loss=1.006580 accuracy=0.585919\n",
      "epoch 83/100: train loss=1.009802 accuracy=0.584863\n",
      "epoch 84/100: train loss=1.005115 accuracy=0.589928\n",
      "epoch 85/100: train loss=1.005331 accuracy=0.589448\n",
      "epoch 86/100: train loss=1.004701 accuracy=0.589470\n",
      "epoch 87/100: train loss=1.004329 accuracy=0.589031\n",
      "epoch 88/100: train loss=1.002931 accuracy=0.589307\n",
      "epoch 89/100: train loss=1.003931 accuracy=0.590028\n",
      "epoch 90/100: train loss=1.006288 accuracy=0.586442\n",
      "epoch 91/100: train loss=1.006592 accuracy=0.589398\n",
      "epoch 92/100: train loss=1.001463 accuracy=0.589774\n",
      "epoch 93/100: train loss=1.004308 accuracy=0.588416\n",
      "epoch 94/100: train loss=1.005757 accuracy=0.586883\n",
      "epoch 95/100: train loss=0.999448 accuracy=0.591624\n",
      "epoch 96/100: train loss=1.001314 accuracy=0.588085\n",
      "epoch 97/100: train loss=1.004636 accuracy=0.586345\n",
      "epoch 98/100: train loss=1.000279 accuracy=0.589456\n",
      "epoch 99/100: train loss=0.999808 accuracy=0.589819\n",
      "epoch 100/100: train loss=1.002294 accuracy=0.585584\n"
     ]
    }
   ],
   "source": [
    "def train(data, iz, batchsize=10000, nepoch=100, seed=123):\n",
    "    \n",
    "    features = jnp.asarray(data)\n",
    "    labels = jnp.asarray(iz, jnp.int32)\n",
    "    ndata = len(features)\n",
    "    nbatch = ndata // batchsize\n",
    "    gen = np.random.RandomState(seed)\n",
    "    \n",
    "    _, initial_params = Classifier.init(jax.random.PRNGKey(0), features[:batchsize])\n",
    "    model = nn.Model(Classifier, initial_params)\n",
    "    print(jax.tree_map(jnp.shape, model.params))\n",
    "    optimizer = optim.Momentum(learning_rate=0.001, beta=0.9).create(model)\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        batch = gen.choice(ndata, nbatch * batchsize, replace=False).reshape(nbatch, batchsize)\n",
    "        for i in range(nbatch):\n",
    "            optimizer = train_step(optimizer, {'features': features[batch[i]], 'labels': labels[batch[i]]})\n",
    "        train_eval = eval_step(optimizer.target, {'features': features, 'labels': labels})\n",
    "        print(f'epoch {epoch+1}/{nepoch}: train loss={train_eval[\"loss\"]:.6f} accuracy={train_eval[\"accuracy\"]:.6f}')\n",
    "\n",
    "train(data, label8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = 10000\n",
    "features = jnp.array(features[:ndata])\n",
    "labels = jnp.array(train_z[:ndata])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def apply(self, x, nbins):\n",
    "        x = nn.Dense(x, 100, name='L1')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, 100, name='L2')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, nbins, name='L3')\n",
    "        return nn.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = NN.partial(nbins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, nn_init = module.init(jax.random.PRNGKey(0), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': {'bias': (100,), 'kernel': (4, 100)},\n",
       " 'L2': {'bias': (100,), 'kernel': (100, 100)},\n",
       " 'L3': {'bias': (4,), 'kernel': (100, 4)}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(jnp.shape, nn_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Model(module, nn_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': {'bias': (100,), 'kernel': (4, 100)},\n",
       " 'L2': {'bias': (100,), 'kernel': (100, 100)},\n",
       " 'L3': {'bias': (4,), 'kernel': (100, 4)}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(jnp.shape, model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.20889738, 0.25362805, 0.26156127, 0.27591333], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learning_rate=0.001).create(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(optimizer, batch):\n",
    "    \n",
    "    def loss_fn(model):\n",
    "        out = model(batch['features'])\n",
    "        idx = jnp.argmax(out, axis=-1)\n",
    "        print(idx)\n",
    "        scores = get_binned_scores(idx, batch['z'], *init_data)\n",
    "        print(scores)\n",
    "        return scores['FOM_DETF_3x2']\n",
    "    \n",
    "    loss, g = jax.value_and_grad(loss_fn)(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(g)\n",
    "    return optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 10\n",
    "gen = np.random.RandomState(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    idx = gen.choice(ndata, nbatch)\n",
    "    return {'features': features[idx], 'z': labels[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 2 0 3 1]\n",
      "{'SNR_3x2': DeviceArray(1319.0688, dtype=float32), 'FOM_3x2': DeviceArray(2835.5964, dtype=float32), 'FOM_DETF_3x2': DeviceArray(29.425533, dtype=float32)}\n",
      "29.425533\n",
      "[0 3 3 3 3 0 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 2 3 3 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 2 3 3 3 3 3 3 3 2]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 2 2 2 2 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 3 2 2 3 3 3 2]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 2 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[2 2 3 3 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 3 2 2 3 3 2 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "def train(opt, niter=10):\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(niter):\n",
    "        opt, loss = train_step(opt, get_batch())\n",
    "        print(loss)\n",
    "        \n",
    "train(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
