{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classifier Random\n",
      "Found classifier ZotBin\n",
      "Found classifier RandomForest\n",
      "Found classifier IBandOnly\n"
     ]
    }
   ],
   "source": [
    "from tomo_challenge import load_data, load_redshift\n",
    "from tomo_challenge.jax_metrics import ell_binning, compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nn, optim, serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize fast metric calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zotbin.binned import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data = load_binned('binned_40.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the challenge data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands='riz'\n",
    "include_colors=False\n",
    "include_errors=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5410171 training rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkirkby/DESC/tomo/tomo_challenge/data.py:89: UserWarning: Setting inf (undetected) bands to mag=30\n",
      "  warnings.warn(\"Setting inf (undetected) bands to mag=30\")\n"
     ]
    }
   ],
   "source": [
    "train_file='/media/data2/tomo_challenge_data/ugrizy_buzzard/training.hdf5'\n",
    "train_data = load_data(train_file, bands, \n",
    "                       errors=include_errors,\n",
    "                       colors=include_colors, array=True)\n",
    "train_z = load_redshift(train_file)\n",
    "print(f'Loaded {len(train_data)} training rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10817982 validation rows.\n"
     ]
    }
   ],
   "source": [
    "validation_file='/media/data2/tomo_challenge_data/ugrizy_buzzard/validation.hdf5'\n",
    "valid_data = load_data(validation_file, bands, \n",
    "                       errors=include_errors,\n",
    "                       colors=include_colors, array=True)\n",
    "valid_z = load_redshift(validation_file)\n",
    "print(f'Loaded {len(valid_data)} validation rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_z, bins=np.linspace(0, 3, 100), density=True, label='TRAIN', histtype='step')\n",
    "plt.hist(valid_z, bins=np.linspace(0, 3, 100), density=True, label='VALID', histtype='step');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(data, colors=True, pca=True, preprocessor=None):\n",
    "    features = data\n",
    "    if colors:\n",
    "        # Add colors to the raw magnitudes.\n",
    "        features = np.concatenate((features, np.diff(data, axis=1)), axis=1)\n",
    "    if pca:\n",
    "        print('Initializing PCA')\n",
    "        pca = PCA(n_components=data.shape[1]).fit(features)\n",
    "        print('Adding PCA')\n",
    "        principal = pca.transform(features)\n",
    "        features = np.concatenate((features, principal), axis=1)\n",
    "    if preprocessor is None:\n",
    "        print('Initializing preprocessor')\n",
    "        preprocessor = RobustScaler()\n",
    "        preprocessor.fit(features)\n",
    "    return preprocessor.transform(features), preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PCA\n",
      "Adding PCA\n",
      "Initializing preprocessor\n"
     ]
    }
   ],
   "source": [
    "train_features, preproc = prepare_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PCA\n",
      "Adding PCA\n"
     ]
    }
   ],
   "source": [
    "valid_features, _ = prepare_features(valid_data, preprocessor=preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5410171, 8), (10817982, 8))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, valid_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load classification labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2 = np.load('buzzard_labels2.npy')\n",
    "label4 = np.load('buzzard_labels4.npy')\n",
    "label8 = np.load('buzzard_labels8.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    plt.hist(train_z[label8 == i], bins=np.linspace(0, 3, 100), alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dense classifier network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def apply(self, x):\n",
    "        x = nn.Dense(x, features=100, name='L1')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, features=100, name='L2')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, features=100, name='L3')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, features=8, name='Out')\n",
    "        x = nn.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def cross_entropy_loss(logits, label):\n",
    "    return -logits[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(optimizer, batch):\n",
    "    def loss_fn(model):\n",
    "        logits = model(batch['features'])\n",
    "        loss = jnp.mean(cross_entropy_loss(logits, batch['labels']))\n",
    "        return loss\n",
    "    grad = jax.grad(loss_fn)(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(grad)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(model, eval_ds):\n",
    "    logits = model(eval_ds['features'])\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == eval_ds['labels'])\n",
    "    loss = jnp.mean(cross_entropy_loss(logits, eval_ds['labels']))\n",
    "    return {'loss': loss,  'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, lr=0.001, beta=0.9, batchsize=10000, nepoch=100, seed=123):\n",
    "    \n",
    "    features = jnp.asarray(features)\n",
    "    labels = jnp.asarray(labels, jnp.int32)\n",
    "    ndata = len(features)\n",
    "    nbatch = ndata // batchsize\n",
    "    gen = np.random.RandomState(seed)\n",
    "    \n",
    "    _, initial_params = Classifier.init(jax.random.PRNGKey(0), features[:batchsize])\n",
    "    model = nn.Model(Classifier, initial_params)\n",
    "    print(jax.tree_map(jnp.shape, model.params))\n",
    "    #optimizer = optim.Momentum(learning_rate=lr, beta=beta).create(model)\n",
    "    optimizer = optim.Adam(learning_rate=lr, beta1=beta).create(model)\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        batch = gen.choice(ndata, nbatch * batchsize, replace=False).reshape(nbatch, batchsize)\n",
    "        for i in range(nbatch):\n",
    "            optimizer = train_step(optimizer, {'features': features[batch[i]], 'labels': labels[batch[i]]})\n",
    "        train_eval = eval_step(optimizer.target, {'features': features, 'labels': labels})\n",
    "        print(f\"epoch {epoch+1}/{nepoch}: train loss={train_eval['loss']:.6f} accuracy={train_eval['accuracy']:.6f}\")\n",
    "    # Calculate final scores on the validation data.\n",
    "    nvalid = 10000\n",
    "    logits = optimizer.target(valid_features[:nvalid])\n",
    "    valid_labels = jnp.argmax(logits, axis=-1)\n",
    "    #scores = compute_scores(valid_labels, valid_z[:nvalid], metrics=['SNR_3x2', 'FOM_3x2', 'FOM_DETF_3x2'])\n",
    "    #print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1': {'bias': (100,), 'kernel': (8, 100)}, 'L2': {'bias': (100,), 'kernel': (100, 100)}, 'L3': {'bias': (100,), 'kernel': (100, 100)}, 'Out': {'bias': (8,), 'kernel': (100, 8)}}\n",
      "epoch 1/25: train loss=0.922596 accuracy=0.610693\n",
      "epoch 2/25: train loss=0.905724 accuracy=0.614748\n",
      "epoch 3/25: train loss=0.899725 accuracy=0.616237\n",
      "epoch 4/25: train loss=0.897405 accuracy=0.617127\n",
      "epoch 5/25: train loss=0.894007 accuracy=0.617421\n",
      "epoch 6/25: train loss=0.892651 accuracy=0.617482\n",
      "epoch 7/25: train loss=0.892674 accuracy=0.617876\n",
      "epoch 8/25: train loss=0.888223 accuracy=0.619265\n",
      "epoch 9/25: train loss=0.887999 accuracy=0.619324\n",
      "epoch 10/25: train loss=0.887477 accuracy=0.619587\n",
      "epoch 11/25: train loss=0.887147 accuracy=0.620050\n",
      "epoch 12/25: train loss=0.887621 accuracy=0.618943\n",
      "epoch 13/25: train loss=0.886701 accuracy=0.619497\n",
      "epoch 14/25: train loss=0.885256 accuracy=0.620413\n",
      "epoch 15/25: train loss=0.885207 accuracy=0.620071\n",
      "epoch 16/25: train loss=0.884379 accuracy=0.620237\n",
      "epoch 17/25: train loss=0.884016 accuracy=0.620513\n",
      "epoch 18/25: train loss=0.883586 accuracy=0.620398\n",
      "epoch 19/25: train loss=0.883587 accuracy=0.621105\n",
      "epoch 20/25: train loss=0.883700 accuracy=0.620655\n",
      "epoch 21/25: train loss=0.882602 accuracy=0.621058\n",
      "epoch 22/25: train loss=0.882794 accuracy=0.620780\n",
      "epoch 23/25: train loss=0.882781 accuracy=0.620804\n",
      "epoch 24/25: train loss=0.883293 accuracy=0.621076\n",
      "epoch 25/25: train loss=0.882664 accuracy=0.620404\n"
     ]
    }
   ],
   "source": [
    "train(train_features, label8, lr=0.001, nepoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1': {'bias': (100,), 'kernel': (8, 100)}, 'L2': {'bias': (100,), 'kernel': (100, 100)}, 'L3': {'bias': (8,), 'kernel': (100, 8)}}\n",
      "epoch 1/25: train loss=0.903163 accuracy=0.614542\n",
      "epoch 2/25: train loss=0.898913 accuracy=0.615908\n",
      "epoch 3/25: train loss=0.894257 accuracy=0.615906\n",
      "epoch 4/25: train loss=0.892889 accuracy=0.617070\n",
      "epoch 5/25: train loss=0.888801 accuracy=0.619136\n",
      "epoch 6/25: train loss=0.889233 accuracy=0.619295\n",
      "epoch 7/25: train loss=0.887600 accuracy=0.619112\n",
      "epoch 8/25: train loss=0.886174 accuracy=0.619626\n",
      "epoch 9/25: train loss=0.885754 accuracy=0.619567\n",
      "epoch 10/25: train loss=0.885733 accuracy=0.619554\n",
      "epoch 11/25: train loss=0.888255 accuracy=0.619026\n",
      "epoch 12/25: train loss=0.885853 accuracy=0.619142\n",
      "epoch 13/25: train loss=0.886501 accuracy=0.619118\n",
      "epoch 14/25: train loss=0.885347 accuracy=0.618829\n",
      "epoch 15/25: train loss=0.885300 accuracy=0.618889\n",
      "epoch 16/25: train loss=0.884241 accuracy=0.619364\n",
      "epoch 17/25: train loss=0.885148 accuracy=0.619399\n",
      "epoch 18/25: train loss=0.883343 accuracy=0.620250\n",
      "epoch 19/25: train loss=0.885092 accuracy=0.619373\n",
      "epoch 20/25: train loss=0.883568 accuracy=0.619220\n",
      "epoch 21/25: train loss=0.882928 accuracy=0.620596\n",
      "epoch 22/25: train loss=0.883787 accuracy=0.619456\n",
      "epoch 23/25: train loss=0.883013 accuracy=0.619555\n",
      "epoch 24/25: train loss=0.883150 accuracy=0.619888\n",
      "epoch 25/25: train loss=0.882850 accuracy=0.620478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lax/lax.py:5905: UserWarning: Explicitly requested dtype <class 'jax.numpy.lax_numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SNR_3x2': 1528.724853515625, 'FOM_3x2': 5182.98291015625, 'FOM_DETF_3x2': 77.8504638671875}\n"
     ]
    }
   ],
   "source": [
    "train(train_features, label8, lr=0.01, nepoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1': {'bias': (100,), 'kernel': (5, 100)}, 'L2': {'bias': (100,), 'kernel': (100, 100)}, 'L3': {'bias': (8,), 'kernel': (100, 8)}}\n",
      "epoch 1/25: train loss=0.961995 accuracy=0.599905\n",
      "epoch 2/25: train loss=0.930175 accuracy=0.609277\n",
      "epoch 3/25: train loss=0.916610 accuracy=0.613058\n",
      "epoch 4/25: train loss=0.909615 accuracy=0.614050\n",
      "epoch 5/25: train loss=0.906061 accuracy=0.614354\n",
      "epoch 6/25: train loss=0.901649 accuracy=0.616335\n",
      "epoch 7/25: train loss=0.899719 accuracy=0.616357\n",
      "epoch 8/25: train loss=0.897166 accuracy=0.617310\n",
      "epoch 9/25: train loss=0.895917 accuracy=0.617580\n",
      "epoch 10/25: train loss=0.895414 accuracy=0.617646\n",
      "epoch 11/25: train loss=0.893363 accuracy=0.618402\n",
      "epoch 12/25: train loss=0.895605 accuracy=0.616679\n",
      "epoch 13/25: train loss=0.893294 accuracy=0.617549\n",
      "epoch 14/25: train loss=0.892446 accuracy=0.617936\n",
      "epoch 15/25: train loss=0.891476 accuracy=0.618080\n",
      "epoch 16/25: train loss=0.890085 accuracy=0.618846\n",
      "epoch 17/25: train loss=0.889505 accuracy=0.619507\n",
      "epoch 18/25: train loss=0.889445 accuracy=0.618813\n",
      "epoch 19/25: train loss=0.889362 accuracy=0.619434\n",
      "epoch 20/25: train loss=0.888639 accuracy=0.619678\n",
      "epoch 21/25: train loss=0.889461 accuracy=0.618699\n",
      "epoch 22/25: train loss=0.889683 accuracy=0.618813\n",
      "epoch 23/25: train loss=0.888386 accuracy=0.619633\n",
      "epoch 24/25: train loss=0.888906 accuracy=0.619579\n",
      "epoch 25/25: train loss=0.886896 accuracy=0.620176\n",
      "{'SNR_3x2': 1520.98828125, 'FOM_3x2': 5010.44775390625, 'FOM_DETF_3x2': 76.9563217163086}\n"
     ]
    }
   ],
   "source": [
    "train(features2, label8, lr=0.001, nepoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1': {'bias': (100,), 'kernel': (5, 100)}, 'L2': {'bias': (100,), 'kernel': (100, 100)}, 'L3': {'bias': (8,), 'kernel': (100, 8)}}\n",
      "epoch 1/100: train loss=1.344506 accuracy=0.502946\n",
      "epoch 2/100: train loss=1.211980 accuracy=0.507931\n",
      "epoch 3/100: train loss=1.151192 accuracy=0.518618\n",
      "epoch 4/100: train loss=1.114971 accuracy=0.534782\n",
      "epoch 5/100: train loss=1.091149 accuracy=0.545790\n",
      "epoch 6/100: train loss=1.073817 accuracy=0.551027\n",
      "epoch 7/100: train loss=1.060467 accuracy=0.555895\n",
      "epoch 8/100: train loss=1.049600 accuracy=0.560309\n",
      "epoch 9/100: train loss=1.040583 accuracy=0.564981\n",
      "epoch 10/100: train loss=1.032976 accuracy=0.568808\n",
      "epoch 11/100: train loss=1.026528 accuracy=0.573087\n",
      "epoch 12/100: train loss=1.020986 accuracy=0.576905\n",
      "epoch 13/100: train loss=1.015984 accuracy=0.580217\n",
      "epoch 14/100: train loss=1.011693 accuracy=0.582451\n",
      "epoch 15/100: train loss=1.007881 accuracy=0.584231\n",
      "epoch 16/100: train loss=1.004379 accuracy=0.586144\n",
      "epoch 17/100: train loss=1.001204 accuracy=0.586927\n",
      "epoch 18/100: train loss=0.998261 accuracy=0.587729\n",
      "epoch 19/100: train loss=0.995538 accuracy=0.588762\n",
      "epoch 20/100: train loss=0.993044 accuracy=0.589500\n",
      "epoch 21/100: train loss=0.990636 accuracy=0.590291\n",
      "epoch 22/100: train loss=0.988368 accuracy=0.590969\n",
      "epoch 23/100: train loss=0.986243 accuracy=0.591326\n",
      "epoch 24/100: train loss=0.984200 accuracy=0.592457\n",
      "epoch 25/100: train loss=0.982227 accuracy=0.592730\n",
      "epoch 26/100: train loss=0.980462 accuracy=0.593452\n",
      "epoch 27/100: train loss=0.978772 accuracy=0.594029\n",
      "epoch 28/100: train loss=0.977149 accuracy=0.594706\n",
      "epoch 29/100: train loss=0.975606 accuracy=0.594920\n",
      "epoch 30/100: train loss=0.974149 accuracy=0.595542\n",
      "epoch 31/100: train loss=0.972731 accuracy=0.596171\n",
      "epoch 32/100: train loss=0.971386 accuracy=0.596415\n",
      "epoch 33/100: train loss=0.970051 accuracy=0.597279\n",
      "epoch 34/100: train loss=0.968767 accuracy=0.597737\n",
      "epoch 35/100: train loss=0.967596 accuracy=0.598021\n",
      "epoch 36/100: train loss=0.966367 accuracy=0.598687\n",
      "epoch 37/100: train loss=0.965221 accuracy=0.599239\n",
      "epoch 38/100: train loss=0.964116 accuracy=0.599627\n",
      "epoch 39/100: train loss=0.963012 accuracy=0.600081\n",
      "epoch 40/100: train loss=0.962049 accuracy=0.600524\n",
      "epoch 41/100: train loss=0.960976 accuracy=0.601084\n",
      "epoch 42/100: train loss=0.959977 accuracy=0.601545\n",
      "epoch 43/100: train loss=0.959018 accuracy=0.601909\n",
      "epoch 44/100: train loss=0.958072 accuracy=0.602358\n",
      "epoch 45/100: train loss=0.957231 accuracy=0.602714\n",
      "epoch 46/100: train loss=0.956325 accuracy=0.603126\n",
      "epoch 47/100: train loss=0.955431 accuracy=0.603522\n",
      "epoch 48/100: train loss=0.954551 accuracy=0.603994\n",
      "epoch 49/100: train loss=0.953714 accuracy=0.604215\n",
      "epoch 50/100: train loss=0.952874 accuracy=0.604568\n",
      "epoch 51/100: train loss=0.952087 accuracy=0.604994\n",
      "epoch 52/100: train loss=0.951293 accuracy=0.605238\n",
      "epoch 53/100: train loss=0.950538 accuracy=0.605874\n",
      "epoch 54/100: train loss=0.949821 accuracy=0.605673\n",
      "epoch 55/100: train loss=0.949075 accuracy=0.605885\n",
      "epoch 56/100: train loss=0.948375 accuracy=0.606241\n",
      "epoch 57/100: train loss=0.947775 accuracy=0.606610\n",
      "epoch 58/100: train loss=0.947009 accuracy=0.606609\n",
      "epoch 59/100: train loss=0.946408 accuracy=0.606829\n",
      "epoch 60/100: train loss=0.945760 accuracy=0.607418\n",
      "epoch 61/100: train loss=0.945088 accuracy=0.607655\n",
      "epoch 62/100: train loss=0.944498 accuracy=0.607873\n",
      "epoch 63/100: train loss=0.943872 accuracy=0.607737\n",
      "epoch 64/100: train loss=0.943275 accuracy=0.607599\n",
      "epoch 65/100: train loss=0.942663 accuracy=0.608240\n",
      "epoch 66/100: train loss=0.942119 accuracy=0.607896\n",
      "epoch 67/100: train loss=0.941546 accuracy=0.608624\n",
      "epoch 68/100: train loss=0.940983 accuracy=0.608225\n",
      "epoch 69/100: train loss=0.940448 accuracy=0.608439\n",
      "epoch 70/100: train loss=0.939924 accuracy=0.608704\n",
      "epoch 71/100: train loss=0.939457 accuracy=0.608617\n",
      "epoch 72/100: train loss=0.938912 accuracy=0.609141\n",
      "epoch 73/100: train loss=0.938384 accuracy=0.609273\n",
      "epoch 74/100: train loss=0.937915 accuracy=0.608879\n",
      "epoch 75/100: train loss=0.937453 accuracy=0.609356\n",
      "epoch 76/100: train loss=0.936944 accuracy=0.609366\n",
      "epoch 77/100: train loss=0.936543 accuracy=0.609766\n",
      "epoch 78/100: train loss=0.936078 accuracy=0.609813\n",
      "epoch 79/100: train loss=0.935592 accuracy=0.609571\n",
      "epoch 80/100: train loss=0.935168 accuracy=0.609491\n",
      "epoch 81/100: train loss=0.934723 accuracy=0.609711\n",
      "epoch 82/100: train loss=0.934362 accuracy=0.609676\n",
      "epoch 83/100: train loss=0.933899 accuracy=0.609787\n",
      "epoch 84/100: train loss=0.933467 accuracy=0.610119\n",
      "epoch 85/100: train loss=0.933130 accuracy=0.610067\n",
      "epoch 86/100: train loss=0.932694 accuracy=0.610173\n",
      "epoch 87/100: train loss=0.932285 accuracy=0.610443\n",
      "epoch 88/100: train loss=0.931912 accuracy=0.610403\n",
      "epoch 89/100: train loss=0.931532 accuracy=0.610612\n",
      "epoch 90/100: train loss=0.931201 accuracy=0.610543\n",
      "epoch 91/100: train loss=0.930863 accuracy=0.610449\n",
      "epoch 92/100: train loss=0.930556 accuracy=0.610326\n",
      "epoch 93/100: train loss=0.930184 accuracy=0.610843\n",
      "epoch 94/100: train loss=0.929727 accuracy=0.610962\n",
      "epoch 95/100: train loss=0.929469 accuracy=0.610837\n",
      "epoch 96/100: train loss=0.929092 accuracy=0.610878\n",
      "epoch 97/100: train loss=0.928720 accuracy=0.611003\n",
      "epoch 98/100: train loss=0.928381 accuracy=0.611165\n",
      "epoch 99/100: train loss=0.928046 accuracy=0.611306\n",
      "epoch 100/100: train loss=0.927792 accuracy=0.611250\n"
     ]
    }
   ],
   "source": [
    "train(features2, label8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1': {'bias': (100,), 'kernel': (5, 100)}, 'L2': {'bias': (100,), 'kernel': (100, 100)}, 'L3': {'bias': (8,), 'kernel': (100, 8)}}\n",
      "epoch 1/100: train loss=0.961947 accuracy=0.599999\n",
      "epoch 2/100: train loss=0.930119 accuracy=0.609243\n",
      "epoch 3/100: train loss=0.916653 accuracy=0.613068\n",
      "epoch 4/100: train loss=0.909556 accuracy=0.614079\n",
      "epoch 5/100: train loss=0.905720 accuracy=0.614434\n",
      "epoch 6/100: train loss=0.901549 accuracy=0.616372\n",
      "epoch 7/100: train loss=0.899326 accuracy=0.616597\n",
      "epoch 8/100: train loss=0.896991 accuracy=0.617336\n",
      "epoch 9/100: train loss=0.895742 accuracy=0.617588\n",
      "epoch 10/100: train loss=0.895193 accuracy=0.617736\n",
      "epoch 11/100: train loss=0.893125 accuracy=0.618440\n",
      "epoch 12/100: train loss=0.895076 accuracy=0.617022\n",
      "epoch 13/100: train loss=0.893161 accuracy=0.617841\n",
      "epoch 14/100: train loss=0.891973 accuracy=0.618333\n",
      "epoch 15/100: train loss=0.891791 accuracy=0.617958\n",
      "epoch 16/100: train loss=0.889731 accuracy=0.619012\n",
      "epoch 17/100: train loss=0.889311 accuracy=0.619710\n",
      "epoch 18/100: train loss=0.889364 accuracy=0.618769\n",
      "epoch 19/100: train loss=0.889025 accuracy=0.619505\n",
      "epoch 20/100: train loss=0.888526 accuracy=0.619612\n",
      "epoch 21/100: train loss=0.889129 accuracy=0.619191\n",
      "epoch 22/100: train loss=0.889150 accuracy=0.618773\n",
      "epoch 23/100: train loss=0.888153 accuracy=0.620032\n",
      "epoch 24/100: train loss=0.888300 accuracy=0.619727\n",
      "epoch 25/100: train loss=0.886656 accuracy=0.620438\n",
      "epoch 26/100: train loss=0.886172 accuracy=0.620488\n",
      "epoch 27/100: train loss=0.887033 accuracy=0.619958\n",
      "epoch 28/100: train loss=0.886419 accuracy=0.620207\n",
      "epoch 29/100: train loss=0.886093 accuracy=0.620574\n",
      "epoch 30/100: train loss=0.885862 accuracy=0.620305\n",
      "epoch 31/100: train loss=0.885916 accuracy=0.620120\n",
      "epoch 32/100: train loss=0.884968 accuracy=0.620935\n",
      "epoch 33/100: train loss=0.885895 accuracy=0.620884\n",
      "epoch 34/100: train loss=0.884748 accuracy=0.620667\n",
      "epoch 35/100: train loss=0.886466 accuracy=0.620536\n",
      "epoch 36/100: train loss=0.884929 accuracy=0.620757\n",
      "epoch 37/100: train loss=0.885421 accuracy=0.620534\n",
      "epoch 38/100: train loss=0.884269 accuracy=0.620754\n",
      "epoch 39/100: train loss=0.884873 accuracy=0.620556\n",
      "epoch 40/100: train loss=0.885089 accuracy=0.619810\n",
      "epoch 41/100: train loss=0.883854 accuracy=0.621064\n",
      "epoch 42/100: train loss=0.883753 accuracy=0.620877\n",
      "epoch 43/100: train loss=0.885062 accuracy=0.620735\n",
      "epoch 44/100: train loss=0.883838 accuracy=0.620885\n",
      "epoch 45/100: train loss=0.883852 accuracy=0.621233\n",
      "epoch 46/100: train loss=0.883352 accuracy=0.620951\n",
      "epoch 47/100: train loss=0.884300 accuracy=0.620276\n",
      "epoch 48/100: train loss=0.883385 accuracy=0.621304\n",
      "epoch 49/100: train loss=0.882848 accuracy=0.621378\n",
      "epoch 50/100: train loss=0.883461 accuracy=0.620935\n",
      "epoch 51/100: train loss=0.882431 accuracy=0.621734\n",
      "epoch 52/100: train loss=0.883141 accuracy=0.621165\n",
      "epoch 53/100: train loss=0.883213 accuracy=0.621330\n",
      "epoch 54/100: train loss=0.882823 accuracy=0.621264\n",
      "epoch 55/100: train loss=0.882852 accuracy=0.621291\n",
      "epoch 56/100: train loss=0.883166 accuracy=0.621340\n",
      "epoch 57/100: train loss=0.882536 accuracy=0.621332\n",
      "epoch 58/100: train loss=0.882811 accuracy=0.621040\n",
      "epoch 59/100: train loss=0.882525 accuracy=0.621616\n",
      "epoch 60/100: train loss=0.882283 accuracy=0.621800\n",
      "epoch 61/100: train loss=0.882721 accuracy=0.621524\n",
      "epoch 62/100: train loss=0.882487 accuracy=0.621445\n",
      "epoch 63/100: train loss=0.882400 accuracy=0.621216\n",
      "epoch 64/100: train loss=0.882401 accuracy=0.621504\n",
      "epoch 65/100: train loss=0.882181 accuracy=0.621756\n",
      "epoch 66/100: train loss=0.881912 accuracy=0.621606\n",
      "epoch 67/100: train loss=0.884105 accuracy=0.620084\n",
      "epoch 68/100: train loss=0.882854 accuracy=0.621183\n",
      "epoch 69/100: train loss=0.882110 accuracy=0.621618\n",
      "epoch 70/100: train loss=0.881595 accuracy=0.621709\n",
      "epoch 71/100: train loss=0.882615 accuracy=0.621223\n",
      "epoch 72/100: train loss=0.883651 accuracy=0.620373\n",
      "epoch 73/100: train loss=0.881450 accuracy=0.621884\n",
      "epoch 74/100: train loss=0.881129 accuracy=0.621721\n",
      "epoch 75/100: train loss=0.881514 accuracy=0.621791\n",
      "epoch 76/100: train loss=0.882179 accuracy=0.621549\n",
      "epoch 77/100: train loss=0.882239 accuracy=0.621622\n",
      "epoch 78/100: train loss=0.882855 accuracy=0.621055\n",
      "epoch 79/100: train loss=0.881733 accuracy=0.621733\n",
      "epoch 80/100: train loss=0.882334 accuracy=0.620890\n",
      "epoch 81/100: train loss=0.882300 accuracy=0.621396\n",
      "epoch 82/100: train loss=0.882141 accuracy=0.621374\n",
      "epoch 83/100: train loss=0.882202 accuracy=0.621141\n",
      "epoch 84/100: train loss=0.882793 accuracy=0.621183\n",
      "epoch 85/100: train loss=0.881437 accuracy=0.621848\n",
      "epoch 86/100: train loss=0.881058 accuracy=0.621795\n",
      "epoch 87/100: train loss=0.880612 accuracy=0.622080\n",
      "epoch 88/100: train loss=0.881937 accuracy=0.621445\n",
      "epoch 89/100: train loss=0.881824 accuracy=0.621228\n",
      "epoch 90/100: train loss=0.881103 accuracy=0.621622\n",
      "epoch 91/100: train loss=0.881183 accuracy=0.621724\n",
      "epoch 92/100: train loss=0.880230 accuracy=0.621835\n",
      "epoch 93/100: train loss=0.880991 accuracy=0.621952\n",
      "epoch 94/100: train loss=0.880690 accuracy=0.622010\n",
      "epoch 95/100: train loss=0.881659 accuracy=0.622001\n",
      "epoch 96/100: train loss=0.880623 accuracy=0.622039\n",
      "epoch 97/100: train loss=0.880235 accuracy=0.622088\n",
      "epoch 98/100: train loss=0.881082 accuracy=0.621649\n",
      "epoch 99/100: train loss=0.879997 accuracy=0.622040\n",
      "epoch 100/100: train loss=0.880236 accuracy=0.622264\n"
     ]
    }
   ],
   "source": [
    "train(features2, label8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = 10000\n",
    "features = jnp.array(features[:ndata])\n",
    "labels = jnp.array(train_z[:ndata])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def apply(self, x, nbins):\n",
    "        x = nn.Dense(x, 100, name='L1')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, 100, name='L2')\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(x, nbins, name='L3')\n",
    "        return nn.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = NN.partial(nbins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, nn_init = module.init(jax.random.PRNGKey(0), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': {'bias': (100,), 'kernel': (4, 100)},\n",
       " 'L2': {'bias': (100,), 'kernel': (100, 100)},\n",
       " 'L3': {'bias': (4,), 'kernel': (100, 4)}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(jnp.shape, nn_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Model(module, nn_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': {'bias': (100,), 'kernel': (4, 100)},\n",
       " 'L2': {'bias': (100,), 'kernel': (100, 100)},\n",
       " 'L3': {'bias': (4,), 'kernel': (100, 4)}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(jnp.shape, model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.20889738, 0.25362805, 0.26156127, 0.27591333], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learning_rate=0.001).create(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(optimizer, batch):\n",
    "    \n",
    "    def loss_fn(model):\n",
    "        out = model(batch['features'])\n",
    "        idx = jnp.argmax(out, axis=-1)\n",
    "        print(idx)\n",
    "        scores = get_binned_scores(idx, batch['z'], *init_data)\n",
    "        print(scores)\n",
    "        return scores['FOM_DETF_3x2']\n",
    "    \n",
    "    loss, g = jax.value_and_grad(loss_fn)(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(g)\n",
    "    return optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 10\n",
    "gen = np.random.RandomState(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    idx = gen.choice(ndata, nbatch)\n",
    "    return {'features': features[idx], 'z': labels[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 2 0 3 1]\n",
      "{'SNR_3x2': DeviceArray(1319.0688, dtype=float32), 'FOM_3x2': DeviceArray(2835.5964, dtype=float32), 'FOM_DETF_3x2': DeviceArray(29.425533, dtype=float32)}\n",
      "29.425533\n",
      "[0 3 3 3 3 0 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 2 3 3 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 2 3 3 3 3 3 3 3 2]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 2 2 2 2 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 3 2 2 3 3 3 2]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 2 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[2 2 3 3 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n",
      "[3 3 3 3 2 2 3 3 2 3]\n",
      "{'SNR_3x2': DeviceArray(nan, dtype=float32), 'FOM_3x2': DeviceArray(nan, dtype=float32), 'FOM_DETF_3x2': DeviceArray(nan, dtype=float32)}\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "def train(opt, niter=10):\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(niter):\n",
    "        opt, loss = train_step(opt, get_batch())\n",
    "        print(loss)\n",
    "        \n",
    "train(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
